{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "LTCnJ8I1m8rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "import pandas as pd\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from tqdm import tqdm\n",
        "import PyPDF2\n",
        "import io"
      ],
      "metadata": {
        "id": "7-SrGzyxmvES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Choose a Domain and Use Case"
      ],
      "metadata": {
        "id": "KRwoGL-3Yma2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YwS6WO3mPl5"
      },
      "outputs": [],
      "source": [
        "def define_domain_and_use_case():\n",
        "    \"\"\"\n",
        "    Define the domain and use case for the RAG chatbot.\n",
        "\n",
        "    Returns:\n",
        "        dict: Domain information and use case description\n",
        "    \"\"\"\n",
        "    domain_info = {\n",
        "        \"domain\": \"Python Programming Education for Children\",\n",
        "        \"target_audience\": \"Children aged 8-12\",\n",
        "        \"use_case\": \"A child-friendly programming assistant that explains Python concepts in simple terms\",\n",
        "        \"value_proposition\": \"\"\"\n",
        "        This project creates a child-friendly Python programming assistant that explains coding concepts\n",
        "        in simple, accessible ways for young learners aged 8-12. Programming education for children is\n",
        "        increasingly important as digital literacy becomes essential, yet many resources use technical\n",
        "        jargon that overwhelms young learners.\n",
        "\n",
        "        Our chatbot transforms complex programming concepts into age-appropriate explanations with\n",
        "        relatable examples and encouraging language. It helps children understand fundamentals like\n",
        "        variables, loops, and functions through analogies to everyday experiences. By making learning\n",
        "        enjoyable and building confidence, this tool supports early coding education and helps develop\n",
        "        computational thinking skills critical for future academic and career success.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    print(f\"Domain: {domain_info['domain']}\")\n",
        "    print(f\"Target Audience: {domain_info['target_audience']}\")\n",
        "    print(f\"Use Case: {domain_info['use_case']}\")\n",
        "    print(\"\\nValue Proposition:\")\n",
        "    print(domain_info['value_proposition'])\n",
        "\n",
        "    return domain_info\n",
        "\n",
        "# Execute Phase 1\n",
        "domain_info = define_domain_and_use_case()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Collect Data with Web Scraping\n"
      ],
      "metadata": {
        "id": "spwVue5mYp49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_and_scrape_data(seed_urls, pdf_urls=None, output_dir=\"web_scraped_data\", min_words=20000):\n",
        "    \"\"\"\n",
        "    Collect data from web sources and organize it for the RAG system.\n",
        "\n",
        "    Args:\n",
        "        seed_urls (list): List of URLs to scrape\n",
        "        pdf_urls (list): List of PDF URLs to scrape\n",
        "        output_dir (str): Directory to save output files\n",
        "        min_words (int): Minimum word count to collect\n",
        "\n",
        "    Returns:\n",
        "        list: All scraped data as a list of dictionaries\n",
        "    \"\"\"\n",
        "    # Create directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Helper functions for cleaning and extraction\n",
        "    def clean_text(text):\n",
        "        text = ' '.join(text.split())\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text\n",
        "\n",
        "    def extract_pdf_text(pdf_content):\n",
        "        try:\n",
        "            pdf_file = io.BytesIO(pdf_content)\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                page_text = page.extract_text() or \"\"\n",
        "                text += page_text + \"\\n\\n\"\n",
        "            return clean_text(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting PDF text: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def is_pdf_url(url):\n",
        "        return url.lower().endswith('.pdf') or '/pdf/' in url.lower()\n",
        "\n",
        "    def scrape_url(url, min_paragraph_length=50):\n",
        "        try:\n",
        "            time.sleep(random.uniform(1, 3))  # Polite delay\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
        "                'Accept': '*/*'\n",
        "            }\n",
        "            print(f\"Scraping: {url}\")\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Handle PDFs\n",
        "            if is_pdf_url(url) or 'application/pdf' in response.headers.get('Content-Type', ''):\n",
        "                print(f\"Processing PDF: {url}\")\n",
        "                content = extract_pdf_text(response.content)\n",
        "                title = os.path.basename(url) or \"PDF Document\"\n",
        "                word_count = len(content.split())\n",
        "                return {'url': url, 'title': title, 'content': content,\n",
        "                        'word_count': word_count, 'status': 'success', 'type': 'pdf'}\n",
        "\n",
        "            # Handle HTML\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            title = soup.title.string if soup.title else \"No Title\"\n",
        "            paragraphs = []\n",
        "            for p in soup.find_all(['p', 'article', 'section', 'div.content', 'div.main']):\n",
        "                text = p.get_text().strip()\n",
        "                if len(text) >= min_paragraph_length:\n",
        "                    paragraphs.append(clean_text(text))\n",
        "            content = '\\n\\n'.join(paragraphs)\n",
        "            word_count = len(content.split())\n",
        "            return {'url': url, 'title': title, 'content': content,\n",
        "                    'word_count': word_count, 'status': 'success', 'type': 'html'}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {str(e)}\")\n",
        "            return {'url': url, 'title': '', 'content': '',\n",
        "                    'word_count': 0, 'status': f'error: {str(e)}', 'type': 'unknown'}\n",
        "\n",
        "    def find_pdf_links(url, html_content):\n",
        "        pdf_links = []\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            for a_tag in soup.find_all('a', href=True):\n",
        "                href = a_tag['href']\n",
        "                if href.lower().endswith('.pdf'):\n",
        "                    if not href.startswith(('http://', 'https://')):\n",
        "                        base_url = url.rstrip('/')\n",
        "                        if href.startswith('/'):\n",
        "                            href = f\"{'/'.join(base_url.split('/')[:3])}{href}\"\n",
        "                        else:\n",
        "                            href = f\"{base_url}/{href}\"\n",
        "                    pdf_links.append(href)\n",
        "        except Exception as e:\n",
        "            print(f\"Error finding PDF links: {str(e)}\")\n",
        "        return pdf_links\n",
        "\n",
        "    # Main scraping logic\n",
        "    if pdf_urls is None:\n",
        "        pdf_urls = []\n",
        "\n",
        "    all_urls = seed_urls + pdf_urls\n",
        "    all_scraped_data = []\n",
        "    total_word_count = 0\n",
        "    pdf_count = 0\n",
        "\n",
        "    # Scrape all URLs\n",
        "    for url in tqdm(all_urls, desc=\"Scraping URLs\"):\n",
        "        result = scrape_url(url)\n",
        "        all_scraped_data.append(result)\n",
        "\n",
        "        # Count PDF if successful\n",
        "        if result['status'] == 'success' and result['type'] == 'pdf':\n",
        "            pdf_count += 1\n",
        "\n",
        "        # Look for PDF links in HTML pages\n",
        "        if result['status'] == 'success' and result['type'] == 'html':\n",
        "            try:\n",
        "                headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "                response = requests.get(url, headers=headers, timeout=10)\n",
        "                pdf_links = find_pdf_links(url, response.content)\n",
        "                for pdf_url in pdf_links:\n",
        "                    if pdf_url not in [item['url'] for item in all_scraped_data]:\n",
        "                        print(f\"Found PDF link: {pdf_url}\")\n",
        "                        pdf_result = scrape_url(pdf_url)\n",
        "                        all_scraped_data.append(pdf_result)\n",
        "                        if pdf_result['status'] == 'success' and pdf_result['type'] == 'pdf':\n",
        "                            pdf_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing PDF links from {url}: {str(e)}\")\n",
        "\n",
        "        # Update word count\n",
        "        if result['status'] == 'success':\n",
        "            total_word_count += result['word_count']\n",
        "\n",
        "    # Save results to files\n",
        "    for result in all_scraped_data:\n",
        "        if result['status'] == 'success':\n",
        "            domain = urlparse(result['url']).netloc\n",
        "            file_type = \"pdf\" if result['type'] == 'pdf' else \"html\"\n",
        "            filename = f\"{domain.replace('.', '_')}_{hash(result['url']) % 10000}.{file_type}.txt\"\n",
        "            filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"URL: {result['url']}\\n\")\n",
        "                f.write(f\"Title: {result['title']}\\n\")\n",
        "                f.write(f\"Type: {result['type']}\\n\")\n",
        "                f.write(f\"Word Count: {result['word_count']}\\n\\n\")\n",
        "                f.write(result['content'])\n",
        "\n",
        "    # Save overview to CSV\n",
        "    csv_file = os.path.join(output_dir, \"all_scraped_content.csv\")\n",
        "    with open(csv_file, 'w', encoding='utf-8', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['url', 'title', 'content', 'word_count', 'status', 'type'])\n",
        "        for item in all_scraped_data:\n",
        "            truncated = item['content'][:1000] + '...' if len(item['content']) > 1000 else item['content']\n",
        "            writer.writerow([item['url'], item['title'], truncated, item['word_count'], item['status'], item.get('type', 'unknown')])\n",
        "\n",
        "    # Save all content to a single file\n",
        "    with open(os.path.join(output_dir, \"all_content.txt\"), 'w', encoding='utf-8') as f:\n",
        "        for item in all_scraped_data:\n",
        "            if item['status'] == 'success':\n",
        "                f.write(f\"--- {item['title']} ({item.get('type', 'unknown')}) ---\\n\\n\")\n",
        "                f.write(item['content'])\n",
        "                f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    # Print statistics\n",
        "    successful_scrapes = sum(1 for item in all_scraped_data if item['status'] == 'success')\n",
        "    failed_scrapes = sum(1 for item in all_scraped_data if item['status'] != 'success')\n",
        "    html_count = sum(1 for item in all_scraped_data if item.get('type') == 'html' and item['status'] == 'success')\n",
        "\n",
        "    print(f\"\\nWeb Scraping Statistics:\")\n",
        "    print(f\"Total URLs scraped: {len(all_scraped_data)}\")\n",
        "    print(f\"Successful scrapes: {successful_scrapes}\")\n",
        "    print(f\"Failed scrapes: {failed_scrapes}\")\n",
        "    print(f\"HTML pages: {html_count}\")\n",
        "    print(f\"PDF documents: {pdf_count}\")\n",
        "    print(f\"Total words scraped: {total_word_count}\")\n",
        "\n",
        "    # Check if we have enough content\n",
        "    if total_word_count < min_words:\n",
        "        print(f\"Warning: Only collected {total_word_count} words, which is less than the target of {min_words}\")\n",
        "\n",
        "    return all_scraped_data\n",
        "\n",
        "# Define seed URLs\n",
        "seed_urls = [\n",
        "    \"https://www.programiz.com/python-programming/variables-constants-literals\",\n",
        "    \"https://www.programiz.com/python-programming/numbers\",\n",
        "    \"https://www.programiz.com/python-programming/if-elif-else\",\n",
        "    \"https://www.programiz.com/python-programming/for-loop\"\n",
        "]\n",
        "\n",
        "pdf_seed_urls = [\n",
        "    \"https://bugs.python.org/file47781/Tutorial_EDIT.pdf\"\n",
        "]\n",
        "\n",
        "# Execute Phase 2\n",
        "all_scraped_data = collect_and_scrape_data(seed_urls, pdf_seed_urls, \"web_scraped_data__\")"
      ],
      "metadata": {
        "id": "Roe7FI2emam5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Preprocess and Chunk the Text\n"
      ],
      "metadata": {
        "id": "IlwXLlcTYwUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_chunk_text(documents, chunk_size=500, chunk_overlap=100, min_chunk_size=100):\n",
        "    \"\"\"\n",
        "    Process and split documents into overlapping chunks for better retrieval.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of document dictionaries\n",
        "        chunk_size (int): Target size of each chunk\n",
        "        chunk_overlap (int): Overlap between chunks\n",
        "        min_chunk_size (int): Minimum acceptable chunk size\n",
        "\n",
        "    Returns:\n",
        "        list: Processed chunks with metadata\n",
        "    \"\"\"\n",
        "\n",
        "    def clean_text(text):\n",
        "        \"\"\"Remove extra whitespace and normalize text\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        text = re.sub(r'\\n+', '\\n', text)\n",
        "        return text\n",
        "\n",
        "    def recursive_text_splitter(text, chunk_size=500, chunk_overlap=100):\n",
        "        \"\"\"Split text into chunks using a hierarchical approach with multiple separators\"\"\"\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"]\n",
        "\n",
        "        def _split(text, sep):\n",
        "            \"\"\"Split text on a specific separator\"\"\"\n",
        "            if sep == \"\":\n",
        "                return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - chunk_overlap)]\n",
        "\n",
        "            chunks, cur_chunk, cur_len = [], [], 0\n",
        "            for part in text.split(sep):\n",
        "                part = (sep + part) if cur_chunk else part\n",
        "                if cur_len + len(part) > chunk_size and cur_chunk:\n",
        "                    joined = \"\".join(cur_chunk)\n",
        "                    chunks.append(joined)\n",
        "                    overlap = joined[-chunk_overlap:] if chunk_overlap < len(joined) else joined\n",
        "                    cur_chunk = [overlap, part]\n",
        "                    cur_len = len(overlap) + len(part)\n",
        "                else:\n",
        "                    cur_chunk.append(part)\n",
        "                    cur_len += len(part)\n",
        "            if cur_chunk:\n",
        "                chunks.append(\"\".join(cur_chunk))\n",
        "            return chunks\n",
        "\n",
        "        def _recursive(text, idx=0):\n",
        "            \"\"\"Recursively try different separators\"\"\"\n",
        "            if len(text) <= chunk_size or idx >= len(separators):\n",
        "                return [text]\n",
        "            split_chunks = _split(text, separators[idx])\n",
        "            result = []\n",
        "            for chunk in split_chunks:\n",
        "                result.extend(_recursive(chunk, idx + 1) if len(chunk) > chunk_size else [chunk])\n",
        "            return result\n",
        "\n",
        "        return _recursive(text)\n",
        "\n",
        "    # Process each document\n",
        "    chunked_documents = []\n",
        "    chunk_id_counter = 0\n",
        "\n",
        "    for doc in tqdm(documents, desc=\"Chunking documents\"):\n",
        "        if doc.get('status') != 'success' or not doc.get('content'):\n",
        "            continue\n",
        "\n",
        "        # Clean the content\n",
        "        cleaned_content = clean_text(doc['content'])\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = recursive_text_splitter(cleaned_content, chunk_size, chunk_overlap)\n",
        "\n",
        "        # Add metadata to each chunk\n",
        "        for chunk_text in chunks:\n",
        "            if len(chunk_text) < min_chunk_size:\n",
        "                continue  # Skip very small chunks\n",
        "\n",
        "            chunked_documents.append({\n",
        "                'content': chunk_text,\n",
        "                'chunk_id': chunk_id_counter,\n",
        "                'source_doc_id': doc.get('id', 'unknown'),\n",
        "                'url': doc.get('url', ''),\n",
        "                'title': doc.get('title', 'Untitled'),\n",
        "                'doc_type': doc.get('type', 'unknown'),\n",
        "                'word_count': len(chunk_text.split())\n",
        "            })\n",
        "            chunk_id_counter += 1\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"Original documents: {len([d for d in documents if d.get('status') == 'success'])}\")\n",
        "    print(f\"After chunking: {len(chunked_documents)} chunks\")\n",
        "    print(f\"Average chunk size: {sum(len(c['content'].split()) for c in chunked_documents) / len(chunked_documents):.1f} words\")\n",
        "\n",
        "    # Save chunks to file\n",
        "    with open(\"web_scraped_data__/chunked_documents.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(chunked_documents, f, indent=2)\n",
        "\n",
        "    return chunked_documents\n",
        "\n",
        "# Execute Phase 3\n",
        "chunked_data = preprocess_and_chunk_text(all_scraped_data)"
      ],
      "metadata": {
        "id": "4_tJAJAknSE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4: Embed the Chunks"
      ],
      "metadata": {
        "id": "MXg-KYNlY4vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks(chunks, model_name=\"all-MiniLM-L6-v2\", batch_size=32, cache_file=None):\n",
        "    \"\"\"\n",
        "    Convert text chunks to vector embeddings.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of document chunks\n",
        "        model_name (str): Name of the sentence transformer model\n",
        "        batch_size (int): Batch size for embedding\n",
        "        cache_file (str): Path to save/load embeddings cache\n",
        "\n",
        "    Returns:\n",
        "        tuple: (chunks with embeddings, embedding model)\n",
        "    \"\"\"\n",
        "    # Initialize embedding model\n",
        "    print(f\"Loading embedding model: {model_name}\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Try to load cached embeddings\n",
        "    if cache_file and os.path.exists(cache_file):\n",
        "        try:\n",
        "            print(f\"Loading cached embeddings from {cache_file}\")\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                cached_data = pickle.load(f)\n",
        "\n",
        "            # Check if cache matches current chunks\n",
        "            if len(cached_data) == len(chunks) and all(c.get('chunk_id') == cached_data[i].get('chunk_id')\n",
        "                                                      for i, c in enumerate(chunks)):\n",
        "                print(\"Using cached embeddings\")\n",
        "                return cached_data, model\n",
        "            else:\n",
        "                print(\"Cache doesn't match current chunks, recalculating embeddings\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading cache: {e}. Recalculating embeddings.\")\n",
        "\n",
        "    # Extract texts for embedding\n",
        "    texts = [chunk['content'] for chunk in chunks]\n",
        "\n",
        "    # Compute embeddings\n",
        "    print(f\"Computing embeddings for {len(texts)} chunks...\")\n",
        "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
        "\n",
        "    # Add embeddings to chunks\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk['embedding'] = embeddings[i].tolist() # Convert numpy array to list\n",
        "\n",
        "    # Save cache if requested\n",
        "    if cache_file:\n",
        "        print(f\"Saving embeddings to cache: {cache_file}\")\n",
        "        os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
        "\n",
        "        # Fix: Pickle might struggle with complex objects within the 'chunks' list.\n",
        "        # It's safer to create a simplified representation for caching.\n",
        "        simplified_chunks = [{k: v for k, v in chunk.items() if k != 'embedding'} for chunk in chunks] # Remove embeddings for pickle\n",
        "\n",
        "        with open(cache_file, 'wb') as f:\n",
        "            pickle.dump(simplified_chunks, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    print(f\"Computed {len(embeddings)} embeddings with dimension {embeddings[0].shape[0]}\")\n",
        "    return chunks, model\n",
        "chunks_with_embeddings, model = embed_chunks(chunked_data)\n"
      ],
      "metadata": {
        "id": "KvtowZsqnbxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 5: Create a Vector Store (ChromaDB)"
      ],
      "metadata": {
        "id": "WV8Y9uPVY_Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(chunks_with_embeddings, model_name=\"all-MiniLM-L6-v2\",\n",
        "                       collection_name=None, persist_directory=None):\n",
        "    \"\"\"\n",
        "    Create a vector store from embedded chunks.\n",
        "\n",
        "    Args:\n",
        "        chunks_with_embeddings (list): Document chunks with embeddings\n",
        "        model_name (str): Name of the embedding model\n",
        "        collection_name (str): Name for the Chroma collection\n",
        "        persist_directory (str): Directory to persist the database\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Chroma client, Chroma collection)\n",
        "    \"\"\"\n",
        "    # Initialize Chroma client\n",
        "    if persist_directory:\n",
        "        print(f\"Initializing persistent Chroma client at: {persist_directory}\")\n",
        "        client = chromadb.PersistentClient(path=persist_directory)\n",
        "    else:\n",
        "        print(\"Initializing in-memory Chroma client\")\n",
        "        client = chromadb.Client()\n",
        "\n",
        "    # Generate collection name if not provided\n",
        "    if not collection_name:\n",
        "        collection_name = f\"python_for_kids_{int(time.time())}\"\n",
        "\n",
        "    # Set up embedding function\n",
        "    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
        "\n",
        "    # Create or get collection\n",
        "    try:\n",
        "        collection = client.get_collection(name=collection_name)\n",
        "        print(f\"Found existing collection: {collection_name}\")\n",
        "    except Exception:\n",
        "        collection = client.create_collection(\n",
        "            name=collection_name,\n",
        "            embedding_function=embedding_fn,\n",
        "            metadata={\"description\": \"Python programming content for children\",\n",
        "                     \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "        )\n",
        "        print(f\"Created new collection: {collection_name}\")\n",
        "\n",
        "    # Prepare data for Chroma\n",
        "    ids = [f\"chunk_{chunk['chunk_id']}\" for chunk in chunks_with_embeddings]\n",
        "    documents = [chunk['content'] for chunk in chunks_with_embeddings]\n",
        "    embeddings = [chunk['embedding'] for chunk in chunks_with_embeddings]\n",
        "    metadatas = [{\n",
        "        'url': chunk.get('url', ''),\n",
        "        'title': chunk.get('title', 'Untitled'),\n",
        "        'chunk_id': str(chunk.get('chunk_id', 0)),\n",
        "        'doc_type': chunk.get('doc_type', 'unknown'),\n",
        "        'word_count': str(chunk.get('word_count', 0)),\n",
        "    } for chunk in chunks_with_embeddings]\n",
        "\n",
        "    # Add data to collection\n",
        "    print(f\"Adding {len(ids)} documents to the collection...\")\n",
        "    collection.add(\n",
        "        ids=ids,\n",
        "        documents=documents,\n",
        "        metadatas=metadatas,\n",
        "        embeddings=embeddings\n",
        "    )\n",
        "\n",
        "    print(f\"Vector store created with {collection.count()} documents\")\n",
        "    return client, collection\n",
        "\n",
        "# Execute Phase 5\n",
        "persist_dir = \"web_scraped_data__/chroma_db\"\n",
        "client, collection = create_vector_store(\n",
        "    chunks_with_embeddings,\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    collection_name=\"python_for_kids\",\n",
        "    persist_directory=persist_dir\n",
        ")"
      ],
      "metadata": {
        "id": "3rxnHPisngzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 6: RAG System - Retrieval and Generation"
      ],
      "metadata": {
        "id": "XF4BNi9sZG9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_chunks(query, collection, n_results=5):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant chunks for a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query\n",
        "        collection: ChromaDB collection\n",
        "        n_results (int): Number of chunks to retrieve\n",
        "\n",
        "    Returns:\n",
        "        list: Relevant chunks with metadata\n",
        "    \"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    retrieved_chunks = []\n",
        "    for i, (doc, metadata, distance) in enumerate(zip(\n",
        "        results['documents'][0],\n",
        "        results['metadatas'][0],\n",
        "        results['distances'][0]\n",
        "    )):\n",
        "        retrieved_chunks.append({\n",
        "            'content': doc,\n",
        "            'metadata': metadata,\n",
        "            'relevance': 1-distance,\n",
        "            'rank': i+1\n",
        "        })\n",
        "\n",
        "    return retrieved_chunks\n",
        "\n",
        "def format_context(chunks):\n",
        "    \"\"\"\n",
        "    Format retrieved chunks into a string for the LLM context.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): Retrieved chunks\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted context\n",
        "    \"\"\"\n",
        "    context_parts = [\n",
        "        f\"Source {i+1} [{chunk['metadata']['title']}]:\\n{chunk['content']}\\n\"\n",
        "        for i, chunk in enumerate(chunks)\n",
        "    ]\n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "def generate_answer(question, context, api_key=None, model=\"mistral-small\", max_tokens=500):\n",
        "    \"\"\"\n",
        "    Generate an answer using an LLM API based on context.\n",
        "\n",
        "    Args:\n",
        "        question (str): User question\n",
        "        context (str): Retrieved context\n",
        "        api_key (str): API key for Mistral or other LLM\n",
        "        model (str): Model identifier\n",
        "        max_tokens (int): Maximum tokens in response\n",
        "\n",
        "    Returns:\n",
        "        str: Generated answer\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        api_key = \"22C686MeYEWCtJZlh0rGqdQSnhPGPN9J\"  # Directly set API key\n",
        "\n",
        "    url = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "\n",
        "    system_prompt = \"\"\"You are a helpful, friendly AI assistant designed to teach Python programming to children.\n",
        "    Your answers should be:\n",
        "    1. Simple and easy to understand for children\n",
        "    2. Encouraging and positive\n",
        "    3. Accurate and based only on the provided context\n",
        "    4. Include simple examples when appropriate\n",
        "\n",
        "    If you don't know the answer based on the context, say so politely and suggest where they might find more information.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Please answer the following question about Python programming for kids.\n",
        "    Use ONLY the information in the context provided below.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION: {question}\"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.5,\n",
        "        \"max_tokens\": max_tokens\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error with LLM API: {str(e)}\")\n",
        "        return f\"\"\"\n",
        "        Error connecting to LLM API: {str(e)}\n",
        "\n",
        "        For this demo, here's a mock response:\n",
        "\n",
        "        Based on the context provided, Python variables are containers that store data values.\n",
        "        You can think of them like labeled boxes where you put different types of information.\n",
        "        For example, if you write 'name = \"Alex\"', you're creating a variable called 'name' that stores the word \"Alex\".\n",
        "        \"\"\"\n",
        "\n",
        "def rag_answer(question, collection, api_key=None, n_chunks=3):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: retrieves context and generates answer.\n",
        "\n",
        "    Args:\n",
        "        question (str): User question\n",
        "        collection: ChromaDB collection\n",
        "        api_key (str): API key for LLM\n",
        "        n_chunks (int): Number of chunks to retrieve\n",
        "\n",
        "    Returns:\n",
        "        dict: Results including question, answer, and context\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing question: {question}\")\n",
        "    print(\"Retrieving relevant context...\")\n",
        "    chunks = retrieve_relevant_chunks(question, collection, n_results=n_chunks)\n",
        "    context = format_context(chunks)\n",
        "    print(f\"Retrieved {len(chunks)} relevant chunks\")\n",
        "\n",
        "    print(\"Generating answer with LLM...\")\n",
        "    answer = generate_answer(question, context, api_key)\n",
        "\n",
        "    return {\n",
        "        'question': question,\n",
        "        'answer': answer,\n",
        "        'context_chunks': chunks,\n",
        "        'full_context': context,\n",
        "        'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "def display_rag_answer(result):\n",
        "    \"\"\"\n",
        "    Display RAG answer in a readable format.\n",
        "\n",
        "    Args:\n",
        "        result (dict): RAG answer result\n",
        "    \"\"\"\n",
        "    from IPython.display import display, Markdown\n",
        "\n",
        "    print(\"Question:\")\n",
        "    display(Markdown(f\"**{result['question']}**\"))\n",
        "\n",
        "    print(\"\\nRelevant Context:\")\n",
        "    structured_content = \"\"\n",
        "    for i, item in enumerate(result['context_chunks']):\n",
        "        structured_content += f\"### Source {i+1}: {item['metadata']['title']}\\n\"\n",
        "        structured_content += f\"**Relevance Score**: {item['relevance']:.4f}\\n\"\n",
        "        structured_content += f\"**Word Count**: {item['metadata']['word_count']}\\n\"\n",
        "        structured_content += f\"[Read More]({item['metadata']['url']})\\n\\n\"\n",
        "        structured_content += f\"{item['content'][:300]}...\\n\\n---\\n\\n\"\n",
        "    display(Markdown(structured_content))\n",
        "\n",
        "    print(\"\\nGenerated Answer:\")\n",
        "    display(Markdown(result['answer']))\n",
        "\n",
        "# Example usage\n",
        "def test_rag_system(collection, questions):\n",
        "    \"\"\"\n",
        "    Test the RAG system with a list of questions.\n",
        "\n",
        "    Args:\n",
        "        collection: ChromaDB collection\n",
        "        questions (list): List of test questions\n",
        "\n",
        "    Returns:\n",
        "        list: Results for each question\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for question in questions:\n",
        "        result = rag_answer(question, collection, api_key=\"22C686MeYEWCtJZlh0rGqdQSnhPGPN9J\")\n",
        "        display_rag_answer(result)\n",
        "        results.append(result)\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "kUR8zsh_Bfc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"How do for loops work in Python?\"\n",
        "result1 = rag_answer(question1, collection)\n",
        "display_rag_answer(result1)"
      ],
      "metadata": {
        "id": "v3VGh9qXEHDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = \"what is if conditions in Python?\"\n",
        "result2 = rag_answer(question2, collection)\n",
        "display_rag_answer(result2)\n"
      ],
      "metadata": {
        "id": "kESkrmHyHe1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question3 = \"What are variables in Python?\"\n",
        "result3 = rag_answer(question3, collection)\n",
        "display_rag_answer(result3)"
      ],
      "metadata": {
        "id": "tp9tUDnfH9kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question4 = \"What types of numbers can I use in Python?\"\n",
        "result4 = rag_answer(question4, collection)\n",
        "display_rag_answer(result4)"
      ],
      "metadata": {
        "id": "UrFz1UuiIlPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question5 = \"How can I make a simple game in Python?\"\n",
        "result5 = rag_answer(question5, collection)\n",
        "display_rag_answer(result5)"
      ],
      "metadata": {
        "id": "ovz2-i9XJGyT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}